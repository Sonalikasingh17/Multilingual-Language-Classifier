{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec18d46",
   "metadata": {},
   "source": [
    "# MASSIVE Dataset Language Classification\n",
    "**This notebook demonstrates language classification tasks using the MASSIVE multilingual dataset.**\n",
    " \n",
    "DATASET_LINK :  https://huggingface.co/datasets/qanastek/MASSIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf00b7c",
   "metadata": {},
   "source": [
    "Tasks Covered:\n",
    "  - Load and inspect the dataset\n",
    "  - Preprocess data from Roman-script locales\n",
    "  - Train Naive Bayes for language classification\n",
    "  - Train LDA/QDA for continent classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bdbee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\IIT M\\Interview_Prep\\Project\\Multilingual_Language_Classifier\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'qanastek/MASSIVE' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n",
      "Using the latest cached version of the dataset since qanastek/MASSIVE couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'en-US' at C:\\Users\\singh\\.cache\\huggingface\\datasets\\qanastek___massive\\en-US\\1.0.0\\31cdffab94ac97bfe5a394b1e96344c96f0ad847e1d796c7562d8c8b449e22e6 (last modified on Sun Jun 15 19:19:13 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'tokens', 'ner_tags', 'worker_id', 'slot_method', 'judgments'],\n",
      "    num_rows: 2974\n",
      "})\n",
      "{'id': '0', 'locale': 'en-US', 'partition': 'test', 'scenario': 9, 'intent': 55, 'utt': 'wake me up at five am this week', 'annot_utt': 'wake me up at [time : five am] [date : this week]', 'tokens': ['wake', 'me', 'up', 'at', 'five', 'am', 'this', 'week'], 'ner_tags': [0, 0, 0, 0, 60, 16, 7, 37], 'worker_id': '1', 'slot_method': {'slot': [], 'method': []}, 'judgments': {'worker_id': [], 'intent_score': [], 'slots_score': [], 'grammar_score': [], 'spelling_score': [], 'language_identification': []}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup directory and languages\n",
    "massive_dataset = load_dataset(\"qanastek/MASSIVE\", \"en-US\", split='test')\n",
    "print(massive_dataset)\n",
    "print(massive_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4b4797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'tokens', 'ner_tags', 'worker_id', 'slot_method', 'judgments'],\n",
      "    num_rows: 2974\n",
      "})\n",
      "{'id': '0', 'locale': 'en-US', 'partition': 'test', 'scenario': 9, 'intent': 55, 'utt': 'wake me up at five am this week', 'annot_utt': 'wake me up at [time : five am] [date : this week]', 'tokens': ['wake', 'me', 'up', 'at', 'five', 'am', 'this', 'week'], 'ner_tags': [0, 0, 0, 0, 60, 16, 7, 37], 'worker_id': '1', 'slot_method': {'slot': [], 'method': []}, 'judgments': {'worker_id': [], 'intent_score': [], 'slots_score': [], 'grammar_score': [], 'spelling_score': [], 'language_identification': []}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Setup directory and languages\n",
    "massive_dataset = load_dataset(\"qanastek/MASSIVE\", \"en-US\", split='test', trust_remote_code=True)\n",
    "print(massive_dataset)\n",
    "print(massive_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a265f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\n",
    "    'af-ZA', 'da-DK', 'de-DE', 'en-US', 'es-ES', 'fr-FR', 'fi-FI', 'hu-HU', 'is-IS', 'it-IT',\n",
    "    'jv-ID', 'lv-LV', 'ms-MY', 'nb-NO', 'nl-NL', 'pl-PL', 'pt-PT', 'ro-RO', 'ru-RU', 'sl-SL',\n",
    "    'sv-SE', 'sq-AL', 'sw-KE', 'tl-PH', 'tr-TR', 'vi-VN', 'cy-GB'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5b4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_lookup = {\n",
    "    'ZA': 'Africa', 'KE': 'Africa', 'AL': 'Europe', 'GB': 'Europe', 'DK': 'Europe', 'DE': 'Europe',\n",
    "    'ES': 'Europe', 'FR': 'Europe', 'FI': 'Europe', 'HU': 'Europe', 'IS': 'Europe', 'IT': 'Europe',\n",
    "    'ID': 'Asia', 'LV': 'Europe', 'MY': 'Asia', 'NO': 'Europe', 'NL': 'Europe', 'PL': 'Europe',\n",
    "    'PT': 'Europe', 'RO': 'Europe', 'RU': 'Europe', 'SL': 'Europe', 'SE': 'Europe', 'PH': 'Asia',\n",
    "    'TR': 'Asia', 'VN': 'Asia', 'US': 'North America'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a74c8a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load all splits\n",
    "\n",
    "def load_massive_split(langs, split):\n",
    "    all_data = []\n",
    "    for lang in langs:\n",
    "        ds = load_dataset(\"qanastek/MASSIVE\", lang, split=split, trust_remote_code=True)\n",
    "        df = pd.DataFrame(ds)\n",
    "        df = df[['locale', 'utt']].copy()\n",
    "        df['split'] = split\n",
    "        all_data.append(df)\n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "train_df = load_massive_split(languages, 'train')\n",
    "val_df = load_massive_split(languages, 'validation')\n",
    "test_df = load_massive_split(languages, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "055c5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train language classifier\n",
    "X_train = train_df['utt']\n",
    "y_train = train_df['locale']\n",
    "\n",
    "X_val = val_df['utt']\n",
    "y_val = val_df['locale']\n",
    "\n",
    "X_test = test_df['utt']\n",
    "y_test = test_df['locale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6242b63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Naive Bayes Language Classification ---\n",
      "Validation Accuracy: 0.9842050609389518\n",
      "Test Accuracy: 0.98399711076241\n",
      "Validation Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       af-ZA       0.91      0.98      0.94      2033\n",
      "       cy-GB       1.00      0.99      0.99      2033\n",
      "       da-DK       0.94      0.96      0.95      2033\n",
      "       de-DE       1.00      0.98      0.99      2033\n",
      "       en-US       0.95      0.99      0.97      2033\n",
      "       es-ES       0.99      0.98      0.98      2033\n",
      "       fi-FI       1.00      0.98      0.99      2033\n",
      "       fr-FR       0.99      0.99      0.99      2033\n",
      "       hu-HU       1.00      0.98      0.99      2033\n",
      "       is-IS       1.00      0.99      0.99      2033\n",
      "       it-IT       0.99      0.99      0.99      2033\n",
      "       jv-ID       0.99      0.98      0.98      2033\n",
      "       lv-LV       1.00      0.99      0.99      2033\n",
      "       ms-MY       0.98      0.99      0.99      2033\n",
      "       nb-NO       0.96      0.94      0.95      2033\n",
      "       nl-NL       0.98      0.97      0.97      2033\n",
      "       pl-PL       0.99      0.98      0.99      2033\n",
      "       pt-PT       0.99      0.99      0.99      2033\n",
      "       ro-RO       1.00      0.99      0.99      2033\n",
      "       ru-RU       1.00      0.99      1.00      2033\n",
      "       sl-SL       1.00      0.99      0.99      2033\n",
      "       sq-AL       1.00      0.99      0.99      2033\n",
      "       sv-SE       0.97      0.98      0.98      2033\n",
      "       sw-KE       1.00      0.99      1.00      2033\n",
      "       tl-PH       0.99      0.99      0.99      2033\n",
      "       tr-TR       1.00      0.99      0.99      2033\n",
      "       vi-VN       1.00      1.00      1.00      2033\n",
      "\n",
      "    accuracy                           0.98     54891\n",
      "   macro avg       0.98      0.98      0.98     54891\n",
      "weighted avg       0.98      0.98      0.98     54891\n",
      "\n",
      "Test Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       af-ZA       0.89      0.98      0.94      2974\n",
      "       cy-GB       1.00      0.99      1.00      2974\n",
      "       da-DK       0.94      0.96      0.95      2974\n",
      "       de-DE       0.99      0.99      0.99      2974\n",
      "       en-US       0.94      0.99      0.96      2974\n",
      "       es-ES       0.98      0.98      0.98      2974\n",
      "       fi-FI       1.00      0.98      0.99      2974\n",
      "       fr-FR       0.99      0.99      0.99      2974\n",
      "       hu-HU       1.00      0.98      0.99      2974\n",
      "       is-IS       1.00      0.99      0.99      2974\n",
      "       it-IT       0.98      0.99      0.98      2974\n",
      "       jv-ID       0.98      0.98      0.98      2974\n",
      "       lv-LV       1.00      0.99      0.99      2974\n",
      "       ms-MY       0.99      0.99      0.99      2974\n",
      "       nb-NO       0.96      0.94      0.95      2974\n",
      "       nl-NL       0.98      0.98      0.98      2974\n",
      "       pl-PL       1.00      0.99      0.99      2974\n",
      "       pt-PT       0.99      0.98      0.99      2974\n",
      "       ro-RO       1.00      0.99      0.99      2974\n",
      "       ru-RU       1.00      0.99      1.00      2974\n",
      "       sl-SL       1.00      0.99      0.99      2974\n",
      "       sq-AL       1.00      0.99      0.99      2974\n",
      "       sv-SE       0.99      0.97      0.98      2974\n",
      "       sw-KE       1.00      0.99      1.00      2974\n",
      "       tl-PH       0.99      0.99      0.99      2974\n",
      "       tr-TR       0.99      0.99      0.99      2974\n",
      "       vi-VN       1.00      1.00      1.00      2974\n",
      "\n",
      "    accuracy                           0.98     80298\n",
      "   macro avg       0.98      0.98      0.98     80298\n",
      "weighted avg       0.98      0.98      0.98     80298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate language model\n",
    "val_preds = pipeline.predict(X_val)\n",
    "test_preds = pipeline.predict(X_test)\n",
    "\n",
    "print(\"\\n--- Naive Bayes Language Classification ---\")\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "print(\"Validation Report:\")\n",
    "print(classification_report(y_val, val_preds))\n",
    "print(\"Test Report:\")\n",
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d5fda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Add continent labels\n",
    "def extract_country(locale):\n",
    "    return locale.split('-')[1]\n",
    "\n",
    "def map_continent(locale):\n",
    "    country = extract_country(locale)\n",
    "    return continent_lookup.get(country, 'Unknown')\n",
    "\n",
    "train_df['continent'] = train_df['locale'].apply(map_continent)\n",
    "val_df['continent'] = val_df['locale'].apply(map_continent)\n",
    "test_df['continent'] = test_df['locale'].apply(map_continent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8e194a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LDA Continent Classification ---\n",
      "Validation Accuracy: 0.8975606201380919\n",
      "Test Accuracy: 0.8945428279658273\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Africa       0.90      0.71      0.79      4066\n",
      "         Asia       0.99      0.67      0.80     10165\n",
      "       Europe       0.88      0.99      0.93     38627\n",
      "North America       0.87      0.73      0.80      2033\n",
      "\n",
      "     accuracy                           0.90     54891\n",
      "    macro avg       0.91      0.77      0.83     54891\n",
      " weighted avg       0.90      0.90      0.89     54891\n",
      "\n",
      "\n",
      "--- QDA Continent Classification ---\n",
      "Validation Accuracy: 0.7955402525004099\n",
      "Test Accuracy: 0.7911405016314229\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Africa       0.62      0.94      0.75      5948\n",
      "         Asia       0.80      0.93      0.86     14870\n",
      "       Europe       0.99      0.73      0.84     56506\n",
      "North America       0.24      0.99      0.38      2974\n",
      "\n",
      "     accuracy                           0.79     80298\n",
      "    macro avg       0.66      0.90      0.71     80298\n",
      " weighted avg       0.90      0.79      0.82     80298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train LDA/QDA for continent classification\n",
    "X_train = train_df['utt']\n",
    "y_train = train_df['continent']\n",
    "X_val = val_df['utt']\n",
    "y_val = val_df['continent']\n",
    "X_test = test_df['utt']\n",
    "y_test = test_df['continent']\n",
    "\n",
    "# Vectorize and reduce\n",
    "vec = TfidfVectorizer()\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_val_vec = vec.transform(X_val)\n",
    "X_test_vec = vec.transform(X_test)\n",
    "\n",
    "svd = TruncatedSVD(n_components=100)\n",
    "X_train_red = svd.fit_transform(X_train_vec)\n",
    "X_val_red = svd.transform(X_val_vec)\n",
    "X_test_red = svd.transform(X_test_vec)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_red, y_train)\n",
    "val_lda = lda.predict(X_val_red)\n",
    "test_lda = lda.predict(X_test_red)\n",
    "\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train_red, y_train)\n",
    "val_qda = qda.predict(X_val_red)\n",
    "test_qda = qda.predict(X_test_red)\n",
    "\n",
    "print(\"\\n--- LDA Continent Classification ---\")\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, val_lda))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_lda))\n",
    "print(classification_report(y_val, val_lda))\n",
    "\n",
    "print(\"\\n--- QDA Continent Classification ---\")\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, val_qda))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_qda))\n",
    "print(classification_report(y_test, test_qda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76289c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and data saved successfully. You can now use the saved models for predictions.\n",
      "Script executed successfully. All models and data are saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Save the models and vectorizer\n",
    "import joblib\n",
    "joblib.dump(pipeline, 'language_classifier.pkl')\n",
    "joblib.dump(vec, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(svd, 'svd_reducer.pkl')\n",
    "joblib.dump(lda, 'lda_model.pkl')\n",
    "joblib.dump(qda, 'qda_model.pkl')\n",
    "# Step 8: Save the dataframes\n",
    "train_df.to_csv('train_data.csv', index=False)\n",
    "val_df.to_csv('val_data.csv', index=False)\n",
    "test_df.to_csv('test_data.csv', index=False)\n",
    "# Save the continent mappings\n",
    "joblib.dump(continent_lookup, 'continent_lookup.pkl')\n",
    "# Save the language mappings\n",
    "language_lookup = {lang: lang.split('-')[0] for lang in languages}\n",
    "joblib.dump(language_lookup, 'language_lookup.pkl')\n",
    "# Save the language and continent mappings\n",
    "language_continent_lookup = {lang: map_continent(lang) for lang in languages}\n",
    "joblib.dump(language_continent_lookup, 'language_continent_lookup.pkl')\n",
    "# Step 9: Print completion message\n",
    "print(\"Models and data saved successfully. You can now use the saved models for predictions.\")\n",
    "# Step 10: Load and test saved models\n",
    "def load_and_test_models():\n",
    "    # Load the models and vectorizer\n",
    "    language_classifier = joblib.load('language_classifier.pkl')\n",
    "    tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "    svd_reducer = joblib.load('svd_reducer.pkl')\n",
    "    lda_model = joblib.load('lda_model.pkl')\n",
    "    qda_model = joblib.load('qda_model.pkl')\n",
    "\n",
    "    # Load the dataframes\n",
    "    train_data = pd.read_csv('train_df.csv')\n",
    "    val_data = pd.read_csv('val_data.csv')\n",
    "    test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "    # Test the language classifier\n",
    "    sample_texts = [\"Hello, how are you?\", \"Bonjour, comment ça va?\", \"Hola, ¿cómo estás?\"]\n",
    "    sample_preds = language_classifier.predict(sample_texts)\n",
    "    print(\"Sample Predictions for Language Classifier:\", sample_preds)\n",
    "    # Test the continent classifier\n",
    "    sample_texts_vec = tfidf_vectorizer.transform(sample_texts)\n",
    "    sample_texts_red = svd_reducer.transform(sample_texts_vec)\n",
    "    lda_preds = lda_model.predict(sample_texts_red)\n",
    "    qda_preds = qda_model.predict(sample_texts_red)\n",
    "    print(\"Sample Predictions for LDA Continent Classifier:\", lda_preds)\n",
    "    print(\"Sample Predictions for QDA Continent Classifier:\", qda_preds)\n",
    "    return sample_preds, lda_preds, qda_preds\n",
    "# Load and test the saved models\n",
    "# load_and_test_models()\n",
    "# Uncomment the line below to test the saved models\n",
    "# load_and_test_models()\n",
    "# Step 11: Finalize the script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script executed successfully. All models and data are saved.\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19110202",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Uncomment the line below to test the saved models\n",
    "    # load_and_test_models()\n",
    "# End of the script \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86cbba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
