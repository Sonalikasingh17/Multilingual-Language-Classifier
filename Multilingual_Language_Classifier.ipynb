{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "522aae4f",
   "metadata": {},
   "source": [
    "## Multilingual Language Classifier ‚Äî End-to-End Notebook  \n",
    "\n",
    "**This notebook demonstrates language classification tasks using the MASSIVE multilingual dataset.**\n",
    " \n",
    "DATASET_LINK :  https://huggingface.co/datasets/qanastek/MASSIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0f067",
   "metadata": {},
   "source": [
    "```bash\n",
    "00-Intro      ‚îÇ project description, imports\n",
    "01-Config     ‚îÇ language list & helper dicts\n",
    "02-DataLoad   ‚îÇ MASSIVE download & split to train/val/test\n",
    "03-EDA        ‚îÇ bar plots of sentence length, class counts\n",
    "04-TrainLang  ‚îÇ TF-IDF + MultinomialNB  \n",
    "05-TrainCont  ‚îÇ TF-IDF + SVD + LDA & QDA \n",
    "06-SaveModels ‚îÇ joblib.dump to artifacts/  (same paths as PredictPipeline)\n",
    "07-Evaluate   ‚îÇ confusion matrices & macro-accuracy on test split\n",
    "08-QuickDemo  ‚îÇ load pickles with your  PredictionPipeline()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad506123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os,json,sys,warnings\n",
    "import joblib,pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix,ConfusionMatrixDisplay, roc_curve, auc)\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\n",
    "                                          \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0549e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directory and languages\n",
    "massive_dataset = load_dataset(\"qanastek/MASSIVE\", \"en-US\", split='test', trust_remote_code=True)\n",
    "print(massive_dataset)\n",
    "print(massive_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARTIFACTS = Path(\"artifacts\")\n",
    "# ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "# Paths\n",
    "ARTIFACTS = \"artifacts\"\n",
    "os.makedirs(ARTIFACTS, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27 Roman-script locales\n",
    "LANGS: List[str] = [\n",
    "    'af-ZA','da-DK','de-DE','en-US','es-ES','fr-FR','fi-FI','hu-HU','is-IS','it-IT',\n",
    "    'jv-ID','lv-LV','ms-MY','nb-NO','nl-NL','pl-PL','pt-PT','ro-RO','ru-RU','sl-SL',\n",
    "    'sv-SE','sq-AL','sw-KE','tl-PH','tr-TR','vi-VN','cy-GB'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c053d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locale ‚Üí continent\n",
    "continent_lookup = {\n",
    "    'ZA':'Africa','KE':'Africa','AL':'Europe','GB':'Europe','DK':'Europe','DE':'Europe',\n",
    "    'ES':'Europe','FR':'Europe','FI':'Europe','HU':'Europe','IS':'Europe','IT':'Europe',\n",
    "    'ID':'Asia','LV':'Europe','MY':'Asia','NO':'Europe','NL':'Europe','PL':'Europe',\n",
    "    'PT':'Europe','RO':'Europe','RU':'Europe','SL':'Europe','SE':'Europe','PH':'Asia',\n",
    "    'TR':'Asia','VN':'Asia','US':'North America'\n",
    "}\n",
    "\n",
    "# continent_lookup = dict(\n",
    "#     ZA=\"Africa\", KE=\"Africa\",\n",
    "#     AL=\"Europe\", GB=\"Europe\", DK=\"Europe\", DE=\"Europe\", ES=\"Europe\", FR=\"Europe\",\n",
    "#     FI=\"Europe\", HU=\"Europe\", IS=\"Europe\", IT=\"Europe\", LV=\"Europe\", NO=\"Europe\",\n",
    "#     NL=\"Europe\", PL=\"Europe\", PT=\"Europe\", RO=\"Europe\", RU=\"Europe\", SL=\"Europe\",\n",
    "#     SE=\"Europe\",\n",
    "#     ID=\"Asia\", MY=\"Asia\", PH=\"Asia\", TR=\"Asia\", VN=\"Asia\",\n",
    "#     US=\"North America\",\n",
    "# )\n",
    "\n",
    "# def to_continent(locale): return CONTINENT.get(locale.split(\"-\")[1], \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc8fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_continent(locale:str)->str:\n",
    "    return continent_lookup[locale.split('-')[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_massive_splits(locales:List[str])->Tuple[pd.DataFrame,pd.DataFrame,pd.DataFrame]:\n",
    "    \"\"\"Loads train/validation/test once per locale (cached).\"\"\"\n",
    "    \n",
    "    buckets = {\"train\":[], \"validation\":[], \"test\":[]}\n",
    "    for loc in locales:\n",
    "        ds = load_dataset(\"qanastek/MASSIVE\", loc, trust_remote_code=True)\n",
    "        for split in buckets:\n",
    "            tmp = pd.DataFrame(ds[split])[[\"utt\"]].copy()\n",
    "            tmp[\"locale\"] = loc\n",
    "            buckets[split].append(tmp)\n",
    "    dfs = {k: pd.concat(v, ignore_index=True) for k,v in buckets.items()}\n",
    "    for df in dfs.values():\n",
    "        df[\"continent\"] = df[\"locale\"].apply(map_continent)\n",
    "    return dfs[\"train\"], dfs[\"validation\"], dfs[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Datasets\n",
    "\n",
    "# def load_split(langs, split):\n",
    "#     frames = []\n",
    "#     for loc in langs:\n",
    "#         ds = load_dataset(\"qanastek/MASSIVE\", loc, split=split, trust_remote_code=True)\n",
    "#         df = pd.DataFrame(ds)[[\"locale\",\"utt\"]]\n",
    "#         df[\"split\"] = split\n",
    "#         frames.append(df)\n",
    "#     return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# train_df = load_split(LANGS,\"train\")\n",
    "# val_df   = load_split(LANGS,\"validation\")\n",
    "# test_df  = load_split(LANGS,\"test\")\n",
    "\n",
    "# for df in (train_df,val_df,test_df):\n",
    "#     df[\"continent\"] = df[\"locale\"].apply(to_continent)\n",
    "\n",
    "# print(train_df.shape, val_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "print(\"üóÉ  Loading MASSIVE splits ... (first run may take ~2-3 min)\")\n",
    "train_df, val_df, test_df = load_massive_splits(LANGS)\n",
    "print(\"Shapes:\", train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA Plots\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "fig1 = plt.figure(figsize=(10,4))\n",
    "sns.countplot(data=train_df, y=\"locale\", order=train_df.locale.value_counts().index)\n",
    "plt.title(\"Train samples per locale\"); plt.tight_layout()\n",
    "fig1.savefig(ARTIFACTS/\"samples_per_locale.png\"); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EDA Plots\n",
    "\n",
    "# plt.figure(figsize=(12,4))\n",
    "# train_df['locale'].value_counts().plot(kind='bar')\n",
    "# plt.title(\"Training sentences per language\")\n",
    "# plt.show()\n",
    "\n",
    "# # Sentence length histogram\n",
    "# lens = train_df['utt'].str.split().apply(len)\n",
    "# plt.hist(lens, bins=30, color='steelblue')\n",
    "# plt.title(\"Token length distribution\")\n",
    "# plt.xlabel(\"words per sentence\"); plt.ylabel(\"frequency\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGE MODEL\n",
    "\n",
    "print(\"\\nüî† Training language classifier ...\")\n",
    "lang_pipe = make_pipeline(\n",
    "    TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(1,3), max_features=10000),\n",
    "    MultinomialNB(alpha=0.1))\n",
    "lang_pipe.fit(train_df.utt, train_df.locale)\n",
    "\n",
    "val_pred = lang_pipe.predict(val_df.utt)\n",
    "test_pred= lang_pipe.predict(test_df.utt)\n",
    "print(\"Validation acc:\", accuracy_score(val_df.locale,val_pred).round(4))\n",
    "print(\"Test acc:\", accuracy_score(test_df.locale,test_pred).round(4))\n",
    "print(classification_report(val_df.locale,val_pred)[:700])\n",
    "\n",
    "cm = confusion_matrix(val_df.locale, val_pred, labels=LANGS)\n",
    "fig2 = plt.figure(figsize=(12,10))\n",
    "sns.heatmap(cm, cmap=\"mako\", cbar=False, xticklabels=False, yticklabels=False)\n",
    "plt.title(\"Language confusion matrix (val)\"); plt.tight_layout()\n",
    "fig2.savefig(ARTIFACTS/\"lang_confusion.png\"); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a3345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONTINENT MODEL \n",
    "\n",
    "print(\"\\nüåç Training continent classifier ...\")\n",
    "vec = TfidfVectorizer(max_features=15000)\n",
    "X_train = vec.fit_transform(train_df.utt)\n",
    "X_val   = vec.transform(val_df.utt)\n",
    "X_test  = vec.transform(test_df.utt)\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "X_train_r = svd.fit_transform(X_train)\n",
    "X_val_r   = svd.transform(X_val)\n",
    "X_test_r  = svd.transform(X_test)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "lda.fit(X_train_r, train_df.continent)\n",
    "qda.fit(X_train_r, train_df.continent)\n",
    "\n",
    "print(\"LDA Validation acc:\", accuracy_score(val_df.continent, lda.predict(X_val_r)).round(4))\n",
    "print(\"LDA Test acc      :\", accuracy_score(test_df.continent, lda.predict(X_test_r)).round(4))\n",
    "print(\"\\n\",classification_report(val_df.continent, lda.predict(X_val_r)))\n",
    "\n",
    "print(\"QDA Validation acc:\", accuracy_score(val_df.continent, qda.predict(X_val_r)).round(4))\n",
    "print(\"QDA Test acc      :\", accuracy_score(test_df.continent, qda.predict(X_test_r)).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for LDA (one-vs-rest)\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "classes = [\"Africa\",\"Asia\",\"Europe\",\"North America\"]\n",
    "y_val_bin = label_binarize(val_df.continent, classes)\n",
    "y_score   = lda.predict_proba(X_val_r)\n",
    "fig3 = plt.figure(figsize=(6,4))\n",
    "for i,c in enumerate(classes):\n",
    "    fpr,tpr,_ = roc_curve(y_val_bin[:,i], y_score[:,i])\n",
    "    plt.plot(fpr,tpr,label=f\"{c} AUC:{auc(fpr,tpr):.2f}\")\n",
    "plt.plot([0,1],[0,1],'k--'); plt.legend(); plt.title(\"LDA ROC curves\"); plt.tight_layout()\n",
    "fig3.savefig(ARTIFACTS/\"lda_roc.png\"); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29724b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE ARTEFACTS \n",
    "print(\"\\nüíæ Saving artefacts to 'artifacts/' ...\")\n",
    "joblib.dump(lang_pipe,          ARTIFACTS/\"language_pipeline.pkl\")\n",
    "joblib.dump(vec,                ARTIFACTS/\"continent_vectorizer.pkl\")\n",
    "joblib.dump(svd,                ARTIFACTS/\"continent_svd.pkl\")\n",
    "joblib.dump(lda,                ARTIFACTS/\"continent_lda_model.pkl\")\n",
    "joblib.dump(qda,                ARTIFACTS/\"continent_qda_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoder for continents\n",
    "\n",
    "import pickle, json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder().fit(classes)\n",
    "joblib.dump(le, ARTIFACTS/\"continent_label_encoder.pkl\")\n",
    "\n",
    "perf = {\n",
    "    \"language_val_acc\": float(accuracy_score(val_df.locale,val_pred)),\n",
    "    \"language_test_acc\": float(accuracy_score(test_df.locale,test_pred)),\n",
    "    \"continent_val_acc\": float(accuracy_score(val_df.continent, lda.predict(X_val_r))),\n",
    "    \"continent_test_acc\": float(accuracy_score(test_df.continent, lda.predict(X_test_r)))\n",
    "}\n",
    "with open(ARTIFACTS/\"model_performance.pkl\",\"wb\") as f: pickle.dump(perf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45babc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  SMOKE TEST \n",
    "print(\"\\nüö¶ Reloading pickles for smoke-test ...\")\n",
    "lang_pipe2 = joblib.load(ARTIFACTS/\"language_pipeline.pkl\")\n",
    "lda2       = joblib.load(ARTIFACTS/\"continent_lda_model.pkl\")\n",
    "vec2       = joblib.load(ARTIFACTS/\"continent_vectorizer.pkl\")\n",
    "svd2       = joblib.load(ARTIFACTS/\"continent_svd.pkl\")\n",
    "le2        = joblib.load(ARTIFACTS/\"continent_label_encoder.pkl\")\n",
    "\n",
    "print(\"Reloaded language test acc:\",\n",
    "      accuracy_score(test_df.locale, lang_pipe2.predict(test_df.utt)).round(4))\n",
    "X_test_r2 = svd2.transform(vec2.transform(test_df.utt))\n",
    "print(\"Reloaded LDA continent test acc:\",\n",
    "      accuracy_score(test_df.continent, lda2.predict(X_test_r2)).round(4))\n",
    "\n",
    "samples = [\"Hello how are you?\",\n",
    "           \"Bonjour, comment √ßa va?\",\n",
    "           \"Guten Tag, wie geht's?\"]\n",
    "print(\"\\nSample predictions:\")\n",
    "for s in samples:\n",
    "    lang = lang_pipe2.predict([s])[0]\n",
    "    cont = le2.inverse_transform([lda2.predict(svd2.transform(vec2.transform([s])))])\n",
    "    print(f\"  '{s[:25]}...' ‚ûú {lang} / {cont}\")\n",
    "\n",
    "print(\"Done ‚úî\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
